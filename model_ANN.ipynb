{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOsdbS1nMChR+k247HTikIl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Classify to TOP Or FAIL"],"metadata":{"id":"c0BlyWZlww00"}},{"cell_type":"markdown","source":["This pipeline demonstrates the implementation and evaluation of an **Artificial Neural Network (ANN)** model for binary classification. The workflow includes:\n","\n","1.  Loading and preprocessing the dataset.\n","2.  Building and training the ANN model with early stopping to prevent overfitting.\n","3.  Performing hyperparameter tuning using RandomizedSearchCV with stratified K-fold cross-validation.\n","4.  Evaluating model performance in terms of accuracy, precision, recall, and F1 score."],"metadata":{"id":"Q0YrIjRPUQsl"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tqdm import tqdm  # Import tqdm for progress bar\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","\n","class ANNClassifier(BaseEstimator, ClassifierMixin):\n","    def __init__(self, input_dim=None, batch_size=32, epochs=20):\n","        \"\"\"Initialize variables for preprocessing, training, and evaluation.\"\"\"\n","        self.input_dim = input_dim\n","        self.batch_size = batch_size\n","        self.epochs = epochs\n","        self.model = None\n","        self.scaler = None\n","        self.x_train, self.x_test, self.y_train, self.y_test = None, None, None, None\n","        self.data = None\n","        self.label_encoders = {}\n","        self.feature_columns = None  # Store feature column names\n","\n","    def load(self, file_path):\n","        \"\"\"Load dataset from a CSV file.\"\"\"\n","        self.data = pd.read_csv(file_path)\n","        print(f\"Data loaded from {file_path}. Shape: {self.data.shape}\")\n","\n","    def preprocess(self, target_column, drop_columns):\n","        \"\"\"Preprocess the dataset: scale features and split data.\"\"\"\n","        print(\"Starting preprocessing...\")\n","\n","        # Separate features and target\n","        X = self.data.drop(columns=drop_columns)\n","        y = self.data[target_column]\n","\n","        # Store feature column names before scaling\n","        self.feature_columns = X.columns\n","\n","        # Encode categorical features\n","        categorical_cols = X.select_dtypes(include=['object']).columns\n","        for col in categorical_cols:\n","            le = LabelEncoder()\n","            X[col] = le.fit_transform(X[col])\n","            self.label_encoders[col] = le  # Store the encoder for future use\n","\n","        # Scale features\n","        self.scaler = StandardScaler()\n","        X_scaled = self.scaler.fit_transform(X)\n","\n","        # Split into training and testing sets (80% train, 20% test)\n","        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","        print(\"Preprocessing completed.\")\n","\n","    def build_model(self):\n","        \"\"\"Build the ANN model.\"\"\"\n","        model = Sequential([\n","            Dense(64, activation='relu', input_dim=self.input_dim),\n","            Dropout(0.3),\n","            Dense(32, activation='relu'),\n","            Dropout(0.2),\n","            Dense(1, activation='sigmoid')\n","        ])\n","        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","        return model\n","\n","    def fit(self, X, y):\n","        \"\"\"Train the ANN model.\"\"\"\n","        print(\"Starting training...\")\n","\n","        # Build the model\n","        self.model = self.build_model()\n","\n","        # Train the model\n","        self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epochs, validation_split=0.2, verbose=1)\n","\n","        print(\"Training completed.\")\n","\n","    def predict(self, X):\n","        \"\"\"Make predictions on new data.\"\"\"\n","        y_pred = (self.model.predict(X) > 0.5).astype(int)\n","        return y_pred\n","\n","    def score(self, X, y):\n","        \"\"\"Evaluate the model performance on the test data.\"\"\"\n","        y_pred = self.predict(X)\n","        accuracy = accuracy_score(y, y_pred)\n","        return accuracy\n","\n","    def evaluate(self):\n","        \"\"\"Evaluate the model on the test set and generate an evaluation summary.\"\"\"\n","        print(\"Starting evaluation...\")\n","\n","        # Predict on test set\n","        y_pred = self.predict(self.x_test)\n","\n","        # Generate evaluation metrics\n","        accuracy = accuracy_score(self.y_test, y_pred)\n","        precision = precision_score(self.y_test, y_pred)\n","        recall = recall_score(self.y_test, y_pred)\n","        f1 = f1_score(self.y_test, y_pred)\n","\n","        print(\"Evaluation Summary:\")\n","        print(f\"Accuracy: {accuracy:.2f}\")\n","        print(f\"Precision: {precision:.2f}\")\n","        print(f\"Recall: {recall:.2f}\")\n","        print(f\"F1 Score: {f1:.2f}\")\n","        print(\"\\nClassification Report:\")\n","        print(classification_report(self.y_test, y_pred))\n","\n","    def final_evaluation(self):\n","        \"\"\"Evaluate the model after hyperparameter tuning.\"\"\"\n","        self.evaluate()\n","\n","    def predict_on_input(self, input_file, drop_columns):\n","        \"\"\"Load and preprocess a new input file, then make predictions.\"\"\"\n","        print(\"Loading and preprocessing prediction input file...\")\n","        input_data = pd.read_csv(input_file)\n","\n","        # Preprocess input data (drop columns that are not needed)\n","        input_data_processed = input_data.drop(columns=drop_columns)\n","\n","        # Encode categorical columns using the saved label encoders\n","        for col, le in self.label_encoders.items():\n","            if col in input_data_processed.columns:\n","                input_data_processed[col] = le.transform(input_data_processed[col])\n","\n","        # Ensure that the features used for prediction match the training data features\n","        input_data_processed = input_data_processed.reindex(columns=self.feature_columns, fill_value=0)\n","\n","        # Scale input data using the same scaler\n","        input_data_scaled = self.scaler.transform(input_data_processed)\n","\n","        # Predict\n","        predictions = self.predict(input_data_scaled)\n","        input_data['success'] = predictions\n","        input_data['success'] = input_data['success'].map({1: 'Top', 0: 'Flop'})\n","\n","        print(\"Predictions completed.\")\n","        return input_data\n","\n","\n","# ==============================================\n","# SECTION 4: EXAMPLE USAGE\n","# ==============================================\n","\n","# Instantiate the classifier\n","pipeline = ANNClassifier(input_dim=4)  # Assuming 4 features\n","\n","# Load and preprocess training data\n","pipeline.load('train_df.csv')\n","label_encoders = pipeline.preprocess(target_column='target', drop_columns=['target'])\n","\n","# Train the model\n","pipeline.fit(pipeline.x_train, pipeline.y_train)\n","\n","# Evaluate the model\n","pipeline.evaluate()\n","\n","# Final evaluation after training\n","pipeline.final_evaluation()\n","\n","# Now, for making predictions on new input data (prediction_input.csv)\n","prediction_output = pipeline.predict_on_input('prediction_input.csv', drop_columns=['item_no'])\n","\n","# Display the predictions\n","print(prediction_output)\n"],"metadata":{"id":"ecWb-ihtKoDk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735840129574,"user_tz":-330,"elapsed":18071,"user":{"displayName":"sameer satpute","userId":"04784341708369533941"}},"outputId":"5bea6a43-775b-4239-962a-071413534816"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Data loaded from train_df.csv. Shape: (10370, 5)\n","Starting preprocessing...\n","Preprocessing completed.\n","Starting training...\n","Epoch 1/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7131 - loss: 0.5790 - val_accuracy: 0.7807 - val_loss: 0.5007\n","Epoch 2/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7568 - loss: 0.5247 - val_accuracy: 0.7807 - val_loss: 0.4971\n","Epoch 3/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7681 - loss: 0.5156 - val_accuracy: 0.7777 - val_loss: 0.4968\n","Epoch 4/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7669 - loss: 0.5165 - val_accuracy: 0.7825 - val_loss: 0.4968\n","Epoch 5/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7759 - loss: 0.5102 - val_accuracy: 0.7795 - val_loss: 0.4957\n","Epoch 6/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7703 - loss: 0.4999 - val_accuracy: 0.7795 - val_loss: 0.4939\n","Epoch 7/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7706 - loss: 0.5142 - val_accuracy: 0.7801 - val_loss: 0.4939\n","Epoch 8/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7692 - loss: 0.5094 - val_accuracy: 0.7831 - val_loss: 0.4918\n","Epoch 9/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7757 - loss: 0.5097 - val_accuracy: 0.7843 - val_loss: 0.4920\n","Epoch 10/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7770 - loss: 0.4998 - val_accuracy: 0.7861 - val_loss: 0.4918\n","Epoch 11/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7806 - loss: 0.4909 - val_accuracy: 0.7837 - val_loss: 0.4896\n","Epoch 12/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7841 - loss: 0.4851 - val_accuracy: 0.7861 - val_loss: 0.4881\n","Epoch 13/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7821 - loss: 0.5002 - val_accuracy: 0.7886 - val_loss: 0.4868\n","Epoch 14/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7790 - loss: 0.4950 - val_accuracy: 0.7880 - val_loss: 0.4874\n","Epoch 15/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7718 - loss: 0.5029 - val_accuracy: 0.7898 - val_loss: 0.4860\n","Epoch 16/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7872 - loss: 0.4842 - val_accuracy: 0.7873 - val_loss: 0.4846\n","Epoch 17/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7828 - loss: 0.4971 - val_accuracy: 0.7831 - val_loss: 0.4833\n","Epoch 18/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7797 - loss: 0.4873 - val_accuracy: 0.7892 - val_loss: 0.4820\n","Epoch 19/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7859 - loss: 0.4864 - val_accuracy: 0.7904 - val_loss: 0.4804\n","Epoch 20/20\n","\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7876 - loss: 0.4853 - val_accuracy: 0.7825 - val_loss: 0.4830\n","Training completed.\n","Starting evaluation...\n","\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","Evaluation Summary:\n","Accuracy: 0.79\n","Precision: 0.79\n","Recall: 0.79\n","F1 Score: 0.79\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.78      0.78      1031\n","           1       0.79      0.79      0.79      1043\n","\n","    accuracy                           0.79      2074\n","   macro avg       0.79      0.79      0.79      2074\n","weighted avg       0.79      0.79      0.79      2074\n","\n","Starting evaluation...\n","\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","Evaluation Summary:\n","Accuracy: 0.79\n","Precision: 0.79\n","Recall: 0.79\n","F1 Score: 0.79\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.78      0.78      1031\n","           1       0.79      0.79      0.79      1043\n","\n","    accuracy                           0.79      2074\n","   macro avg       0.79      0.79      0.79      2074\n","weighted avg       0.79      0.79      0.79      2074\n","\n","Loading and preprocessing prediction input file...\n","\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","Predictions completed.\n","      item_no    category      main_promotion        color  stars success\n","0      405901  Sweatshirt             Catalog         Blue    3.1    Flop\n","1      644275  Polo-Shirt    Frontpage_Header       Yellow    2.6    Flop\n","2      533070       Tunic             Catalog        Green    2.7    Flop\n","3      829436  Polo-Shirt             Catalog       Yellow    2.6    Flop\n","4      801722       Tunic             Catalog       Yellow    4.9     Top\n","...       ...         ...                 ...          ...    ...     ...\n","1995   376794     T-Shirt  Category_Highlight        Green    5.0     Top\n","1996   470289       Tunic             Catalog        Green    2.6    Flop\n","1997   271185  Polo-Shirt             Catalog       Orange    2.7    Flop\n","1998   765041      Blouse    Frontpage_Header  Multi-Color    2.7    Flop\n","1999   985525     T-Shirt  Category_Highlight  Multi-Color    1.5    Flop\n","\n","[2000 rows x 6 columns]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.metrics import classification_report\n","\n","def save_evaluation_metrics_to_csv(pipeline, file_name='ann_evaluation_metrics.csv'):\n","    \"\"\"\n","    Save evaluation metrics and classification report from the pipeline\n","    to a CSV file.\n","    \"\"\"\n","    print(\"Saving evaluation metrics and classification report to CSV...\")\n","\n","    # Predict on test data\n","    y_pred = pipeline.predict(pipeline.x_test)\n","\n","    # Generate summary metrics\n","    accuracy = accuracy_score(pipeline.y_test, y_pred)\n","    precision = precision_score(pipeline.y_test, y_pred)\n","    recall = recall_score(pipeline.y_test, y_pred)\n","    f1 = f1_score(pipeline.y_test, y_pred)\n","\n","    # Create a summary metrics DataFrame\n","    summary_data = {\n","        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"],\n","        \"Value\": [accuracy, precision, recall, f1]\n","    }\n","    summary_df = pd.DataFrame(summary_data)\n","\n","    # Generate a detailed classification report\n","    report_dict = classification_report(pipeline.y_test, y_pred, output_dict=True)\n","    report_df = pd.DataFrame(report_dict).transpose()\n","\n","    # Save both to a CSV file\n","    summary_df.to_csv(file_name, index=False)\n","    detailed_file_name = file_name.replace('.csv', '_detailed.csv')\n","    report_df.to_csv(detailed_file_name)\n","\n","    print(f\"Summary metrics saved to {file_name}.\")\n","    print(f\"Detailed classification report saved to {detailed_file_name}.\")\n","\n","# Call the function\n","save_evaluation_metrics_to_csv(pipeline, file_name='ann_evaluation_metrics.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M9Rlk9ohG_GY","executionInfo":{"status":"ok","timestamp":1735840130058,"user_tz":-330,"elapsed":488,"user":{"displayName":"sameer satpute","userId":"04784341708369533941"}},"outputId":"1cf0f4f1-1c6d-4a71-c938-013c06f20d1a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving evaluation metrics and classification report to CSV...\n","\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","Summary metrics saved to ann_evaluation_metrics.csv.\n","Detailed classification report saved to ann_evaluation_metrics_detailed.csv.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, StratifiedKFold\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.metrics import accuracy_score, classification_report, make_scorer\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from scipy.stats import uniform, randint\n","from tensorflow.keras import layers\n","from tensorflow.keras.callbacks import EarlyStopping\n","import warnings\n","\n","# Suppress warnings\n","warnings.filterwarnings('ignore')\n","\n","# Check GPU availability\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","\n","# Define the ANN Model with improvements\n","class ANNModel(tf.keras.Model):\n","    def __init__(self, neurons=64, dropout_rate=0.3, input_dim=4):\n","        super(ANNModel, self).__init__()\n","        self.dense1 = layers.Dense(neurons, activation='relu', input_dim=input_dim)\n","        self.batch_norm1 = layers.BatchNormalization()\n","        self.dropout1 = layers.Dropout(dropout_rate)\n","        self.dense2 = layers.Dense(int(neurons / 2), activation='relu')\n","        self.batch_norm2 = layers.BatchNormalization()\n","        self.dropout2 = layers.Dropout(dropout_rate)\n","        self.output_layer = layers.Dense(1, activation='sigmoid')\n","\n","    def call(self, inputs):\n","        x = self.dense1(inputs)\n","        x = self.batch_norm1(x)\n","        x = self.dropout1(x)\n","        x = self.dense2(x)\n","        x = self.batch_norm2(x)\n","        x = self.dropout2(x)\n","        return self.output_layer(x)\n","\n","    def compile_model(self):\n","        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","            initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.9)\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n","        self.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Custom Scikit-learn Wrapper for the ANN\n","class CustomANNWrapper(BaseEstimator, ClassifierMixin):\n","    def __init__(self, neurons=64, dropout_rate=0.3, input_dim=4, epochs=10, batch_size=32):\n","        self.neurons = neurons\n","        self.dropout_rate = dropout_rate\n","        self.input_dim = input_dim\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.model = None\n","\n","    def fit(self, X, y):\n","        self.model = ANNModel(neurons=self.neurons, dropout_rate=self.dropout_rate, input_dim=self.input_dim)\n","        self.model.compile_model()\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n","        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n","        return self\n","\n","    def predict(self, X):\n","        return (self.model.predict(X) > 0.5).astype(int)\n","\n","    def score(self, X, y):\n","        y_pred = self.predict(X)\n","        return accuracy_score(y, y_pred)\n","\n","# Load and preprocess the data\n","def load_data(file_path, target_column, drop_columns):\n","    data = pd.read_csv(file_path)\n","    X = data.drop(columns=drop_columns)\n","    y = data[target_column]\n","\n","    # Encode categorical columns\n","    label_encoders = {}\n","    categorical_cols = X.select_dtypes(include=['object']).columns\n","    for col in categorical_cols:\n","        le = LabelEncoder()\n","        X[col] = le.fit_transform(X[col])\n","        label_encoders[col] = le\n","\n","    # Scale features\n","    scaler = StandardScaler()\n","    X_scaled = scaler.fit_transform(X)\n","\n","    return X_scaled, y, label_encoders, scaler\n","\n","# Hyperparameter Tuning using RandomizedSearchCV with K-Fold Cross-Validation\n","def tune_hyperparameters(X, y):\n","    model = CustomANNWrapper()\n","\n","    param_dist = {\n","        'neurons': randint(32, 128),              # Range of neurons\n","        'dropout_rate': uniform(0.2, 0.4),       # Uniform distribution for dropout rate\n","        'epochs': randint(10, 50),               # Range of epochs\n","        'batch_size': [16, 32, 64]               # Fixed batch sizes\n","    }\n","\n","    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","    random_search = RandomizedSearchCV(\n","        estimator=model,\n","        param_distributions=param_dist,\n","        n_iter=20,                               # Number of hyperparameter combinations to try\n","        cv=kfold,                                # K-Fold Cross-Validation\n","        verbose=2,\n","        n_jobs=-1,\n","        scoring=make_scorer(accuracy_score)      # Use accuracy as scoring metric\n","    )\n","    random_search.fit(X, y)\n","\n","    print(\"Best Parameters:\", random_search.best_params_)\n","    print(\"Best Cross-Validation Score:\", random_search.best_score_)\n","\n","    # Return the best model\n","    return random_search.best_estimator_\n","\n","# Evaluate the Model\n","def evaluate_model(model, X_test, y_test):\n","    y_pred = model.predict(X_test)\n","    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, y_pred))\n","\n","# Example Usage\n","if __name__ == \"__main__\":\n","    # Load and preprocess data\n","    X, y, label_encoders, scaler = load_data('train_df.csv', target_column='target', drop_columns=['target'])\n","\n","    # Split data into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Tune hyperparameters with K-Fold Cross-Validation\n","    best_model = tune_hyperparameters(X_train, y_train)\n","\n","    # Evaluate the best model on test data\n","    evaluate_model(best_model, X_test, y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0XcNhZxv6b37","executionInfo":{"status":"ok","timestamp":1735841842018,"user_tz":-330,"elapsed":1711965,"user":{"displayName":"sameer satpute","userId":"04784341708369533941"}},"outputId":"604507de-0b65-4f8e-9239-010eb09fe92e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Num GPUs Available:  0\n","Fitting 5 folds for each of 20 candidates, totalling 100 fits\n","Epoch 1/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7491 - loss: 0.5391 - val_accuracy: 0.7825 - val_loss: 0.4977\n","Epoch 2/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7798 - loss: 0.4945 - val_accuracy: 0.7807 - val_loss: 0.5000\n","Epoch 3/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7839 - loss: 0.4850 - val_accuracy: 0.7861 - val_loss: 0.4884\n","Epoch 4/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7806 - loss: 0.4889 - val_accuracy: 0.7886 - val_loss: 0.4829\n","Epoch 5/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7884 - loss: 0.4829 - val_accuracy: 0.7831 - val_loss: 0.4817\n","Epoch 6/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7929 - loss: 0.4770 - val_accuracy: 0.7904 - val_loss: 0.4747\n","Epoch 7/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7916 - loss: 0.4769 - val_accuracy: 0.8036 - val_loss: 0.4747\n","Epoch 8/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7964 - loss: 0.4669 - val_accuracy: 0.8036 - val_loss: 0.4674\n","Epoch 9/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8118 - loss: 0.4526 - val_accuracy: 0.8012 - val_loss: 0.4677\n","Epoch 10/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8042 - loss: 0.4582 - val_accuracy: 0.7994 - val_loss: 0.4636\n","Epoch 11/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7987 - loss: 0.4649 - val_accuracy: 0.8054 - val_loss: 0.4577\n","Epoch 12/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8032 - loss: 0.4590 - val_accuracy: 0.8054 - val_loss: 0.4637\n","Epoch 13/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8052 - loss: 0.4584 - val_accuracy: 0.8048 - val_loss: 0.4597\n","Epoch 14/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8216 - loss: 0.4332 - val_accuracy: 0.8042 - val_loss: 0.4575\n","Epoch 15/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8023 - loss: 0.4646 - val_accuracy: 0.8090 - val_loss: 0.4545\n","Epoch 16/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8088 - loss: 0.4552 - val_accuracy: 0.8157 - val_loss: 0.4523\n","Epoch 17/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8090 - loss: 0.4434 - val_accuracy: 0.8018 - val_loss: 0.4620\n","Epoch 18/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8120 - loss: 0.4511 - val_accuracy: 0.8060 - val_loss: 0.4574\n","Epoch 19/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8154 - loss: 0.4372 - val_accuracy: 0.8090 - val_loss: 0.4512\n","Epoch 20/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8177 - loss: 0.4474 - val_accuracy: 0.8127 - val_loss: 0.4478\n","Epoch 21/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8198 - loss: 0.4404 - val_accuracy: 0.8114 - val_loss: 0.4554\n","Epoch 22/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8180 - loss: 0.4403 - val_accuracy: 0.8133 - val_loss: 0.4492\n","Epoch 23/24\n","\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8199 - loss: 0.4429 - val_accuracy: 0.8145 - val_loss: 0.4546\n","Epoch 23: early stopping\n","Restoring model weights from the end of the best epoch: 20.\n","Best Parameters: {'batch_size': 16, 'dropout_rate': 0.28306322947332985, 'epochs': 24, 'neurons': 82}\n","Best Cross-Validation Score: 0.8091874187527687\n","\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","Accuracy: 0.8235294117647058\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.78      0.81      1031\n","           1       0.80      0.87      0.83      1043\n","\n","    accuracy                           0.82      2074\n","   macro avg       0.83      0.82      0.82      2074\n","weighted avg       0.83      0.82      0.82      2074\n","\n"]}]},{"cell_type":"markdown","source":["After tuning, the confusion matrix highlights:\n","\n","**True Positives:** Correctly predicted 'Top' classifications.\n","\n","**True Negatives:** Correctly predicted 'Flop' classifications.\n","The model achieves a good balance in handling both classes.\n"],"metadata":{"id":"EoCToiyNUoX6"}},{"cell_type":"markdown","source":["**Conclusion**\n","\n","*  The ANN model demonstrates competitive performance after tuning, with\n","significant improvements in recall and F1 score.\n","\n","*  It is robust for binary classification tasks, especially when non-linear relationships exist in the dataset.\n","\n","*  Further improvements can be explored by integrating more advanced architectures like convolutional or recurrent layers for complex datasets.\n","\n","*  While the ANN is computationally intensive, its scalability and performance make it a strong candidate for classification tasks."],"metadata":{"id":"3F885aJLUs6P"}}]}